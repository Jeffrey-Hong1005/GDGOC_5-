{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorStoreRetrieverMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VectorStoreRetrieverMemory` 는 벡터 스토어에 메모리를 저장하고 호출될 때마다 가장 '눈에 띄는' 상위 K개의 문서를 쿼리합니다.\n",
    "\n",
    "이는 대화내용의 순서를 **명시적으로 추적하지 않는다는 점** 에서 다른 대부분의 메모리 클래스와 다릅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 벡터 스토어를 초기화 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain.docstore import InMemoryDocstore\n",
    "# from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# # 임베딩 모델을 정의합니다.\n",
    "# embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# # Vector Store 를 초기화 합니다.\n",
    "# embedding_size = 1536\n",
    "# index = faiss.IndexFlatL2(embedding_size)\n",
    "# vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 1. faiss 설치 문제\n",
    "# faiss는 window용 공식 wheel제공하지 않음\n",
    "# pip 대신 prebuilt로 설치 가능, but python 버전이 3.11 미만이어야함\n",
    "# langchain은 벡터 스토어가 다양하기 때문에 faiss 대신 Chroma로 대체 \n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"chat_memory\", \n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=None,\n",
    "      \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 사용에서는 `k`를 더 높은 값으로 설정하지만, 여기서는 `k=1` 을 사용하여 다음과 같이 표시합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# # 벡터 조회가 여전히 의미적으로 관련성 있는 정보를 반환한다는 것을 보여주기 위해서입니다.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# memory = VectorStoreRetrieverMemory(retriever=retriever)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 2. Retriever로 변환 (vectorStoreRetrieverMemory에서 사용)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m retriever = \u001b[43mvectorstore\u001b[49m.as_retriever(search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m})\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_dialogue\u001b[39m(human, ai):\n\u001b[32m     12\u001b[39m     vectorstore.add_texts([\n\u001b[32m     13\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHuman: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhuman\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mai\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     ])\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 벡터 조회가 여전히 의미적으로 관련성 있는 정보를 반환한다는 것을 보여주기 위해서입니다.\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "# memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "\n",
    "\n",
    "# 2. Retriever로 변환 (vectorStoreRetrieverMemory에서 사용)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "\n",
    "def save_dialogue(human, ai):\n",
    "    vectorstore.add_texts([\n",
    "        f\"Human: {human}\",\n",
    "        f\"AI: {ai}\"\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# 3. LLM + 프롬프트\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant. Use the memory below if helpful.\\nMemory:\\n{memory}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "# 4. Memory Retrieval → Prompt Binding → 모델 실행 체인\n",
    "\n",
    "def retrieve_memory(question):\n",
    "    docs = retriever.invoke(question)\n",
    "    return \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"memory\": lambda x: retrieve_memory(x[\"question\"]),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "save_dialogue(\n",
    "    \"안녕하세요, 오늘 면접에 참석해주셔서 감사합니다. 자기소개 부탁드립니다.\",\n",
    "    \"안녕하세요. 저는 컴퓨터 과학을 전공한 신입 개발자입니다...\"\n",
    ")\n",
    "\n",
    "save_dialogue(\n",
    "    \"프로젝트에서 어떤 역할을 맡았나요?\",\n",
    "    \"제가 맡은 역할은 백엔드 개발자였습니다...\"\n",
    ")\n",
    "\n",
    "save_dialogue(\n",
    "    \"팀 프로젝트에서 어려움을 겪었던 경험이 있다면?\",\n",
    "    \"초기에 의사소통 문제로 어려움이 있었습니다...\"\n",
    ")\n",
    "\n",
    "save_dialogue(\n",
    "    \"개발자로서 강점이 무엇인가요?\",\n",
    "    \"빠른 학습 능력과 문제 해결 능력입니다...\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음의 질문을 했을 때 Vector Store 로 부터 1개(k=1 이기 때문)의 가장 관련성 높은 대화를 반환합니다.\n",
    "\n",
    "- 질문: \"면접자 전공은 무엇인가요?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 메모리에 질문을 통해 가장 연관성 높은 1개 대화를 추출합니다.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# print(memory.load_memory_variables({\"human\": \"면접자 전공은 무엇인가요?\"})[\"history\"])\u001b[39;00m\n\u001b[32m      4\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33m면접자 전공은 무엇인가요?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mchain\u001b[49m.invoke({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: query})\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "# 메모리에 질문을 통해 가장 연관성 높은 1개 대화를 추출합니다.\n",
    "# print(memory.load_memory_variables({\"human\": \"면접자 전공은 무엇인가요?\"})[\"history\"])\n",
    "\n",
    "query = \"면접자 전공은 무엇인가요?\"\n",
    "response = chain.invoke({\"question\": query})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 다른 질문을 통해 가장 연관성 높은 1개 대화를 추출합니다.\n",
    "\n",
    "- 질문: \"면접자가 프로젝트에서 맡은 역할은 무엇인가요?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     memory.load_memory_variables(\n",
    "#         {\"human\": \"면접자가 프로젝트에서 맡은 역할은 무엇인가요?\"}\n",
    "#     )[\"history\"]\n",
    "# )\n",
    "\n",
    "docs = retriever.invoke(\"면접자가 프로젝트에서 맡은 역할은 무엇인가요?\")\n",
    "history = \"\\n\".join(d.page_content for d in docs)\n",
    "print(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-SGRq4Tjy-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
